# Система распознавания жестов в реальном времени

Эта система использует YOLOv8, MediaPipe и BiLSTM для распознавания динамических жестов русского языка жестов в реальном времени.

## Особенности

- ✅ Извлечение кадров из видео с настраиваемым FPS
- ✅ Обнаружение и отслеживание рук с помощью MediaPipe
- ✅ Извлечение 21 ключевой точки руки (63 признака: x, y, z)
- ✅ Нормализация landmarks относительно запястья
- ✅ Обучение BiLSTM модели с attention механизмом
- ✅ Распознавание динамических жестов в реальном времени
- ✅ Сглаживание предсказаний для стабильности
- ✅ Поддержка GPU (CUDA) для ускорения

## Структура проекта

```
d:/gesture/
├── train/                     # Папка с видео файлами (.mp4)
├── annotations.csv            # Файл аннотаций (attachment_id, text, ...)
├── workdir/                   # Рабочая папка (создается автоматически)
│   ├── frames/               # Извлеченные кадры по классам
│   │   ├── А/               # Кадры для жеста "А"
│   │   ├── Б/               # Кадры для жеста "Б"
│   │   └── .../
│   ├── sequences/           # Последовательности ключевых точек
│   │   ├── *.npy           # Файлы с landmarks
│   │   └── sequences_index.csv
│   ├── frames_meta.csv      # Метаданные кадров
│   └── *.csv               # Статистики
├── models/                   # Обученные модели
│   ├── bilstm_best.pt       # Лучшая модель
│   └── training_history.png # График обучения
├── scripts/                  # Скрипты обработки
│   ├── extract_and_label.py # Извлечение кадров
│   ├── extract_keypoints.py # Извлечение ключевых точек
│   ├── train_lstm.py        # Обучение модели
│   └── realtime_infer.py    # Распознавание в реальном времени
├── requirements.txt          # Зависимости
└── README.md                # Этот файл
```

## Установка

### 1. Клонирование и установка зависимостей

```bash
cd d:/gesture
pip install -r requirements.txt
```

### 2. Проверка данных

Убедитесь, что у вас есть:
- Папка `train/` с видео файлами в формате `{attachment_id}.mp4`
- Файл `annotations.csv` с колонками: `attachment_id`, `text`, `train`

## Использование

### Шаг 1: Извлечение кадров и обнаружение рук

```bash
python scripts/extract_and_label.py
```

Этот скрипт:
- Извлекает кадры из видео (по умолчанию 5 FPS)
- Обнаруживает руки с помощью MediaPipe
- Рисует landmarks и bounding boxes
- Сохраняет кадры по классам в `workdir/frames/`
- Создает метаданные в `workdir/frames_meta.csv`

### Шаг 2: Извлечение последовательностей ключевых точек

```bash
python scripts/extract_keypoints.py
```

Этот скрипт:
- Извлекает 21 ключевую точку руки для каждого кадра
- Нормализует координаты относительно запястья
- Сохраняет последовательности в `workdir/sequences/`
- Создает индекс в `sequences_index.csv`

### Шаг 3: Обучение модели

```bash
python scripts/train_lstm.py
```

Параметры обучения:
- Архитектура: Bidirectional LSTM + Attention
- Длина последовательности: 64 кадра
- Batch size: 32
- Epochs: 50
- Learning rate: 1e-3
- Dropout: 0.3

Модель сохраняется в `models/bilstm_best.pt`

### Шаг 4: Распознавание в реальном времени

```bash
python scripts/realtime_infer.py
```

Параметры командной строки:
```bash
python scripts/realtime_infer.py --help

# Примеры:
python scripts/realtime_infer.py --camera 0 --confidence 0.7
python scripts/realtime_infer.py --model models/bilstm_best.pt --seq_len 64
```

## Параметры и настройки

### extract_and_label.py
- `target_fps`: FPS для извлечения кадров (по умолчанию 5)
- `min_detection_confidence`: Порог обнаружения рук (0.5)
- `min_tracking_confidence`: Порог отслеживания рук (0.5)

### train_lstm.py
- `SEQ_LEN`: Длина последовательности (64)
- `BATCH_SIZE`: Размер батча (32)
- `EPOCHS`: Количество эпох (50)
- `HIDDEN_SIZE`: Размер скрытого слоя LSTM (128)
- `NUM_LAYERS`: Количество слоев LSTM (2)
- `DROPOUT`: Dropout rate (0.3)

### realtime_infer.py
- `--confidence`: Порог уверенности для предсказаний (0.7)
- `--interval`: Интервал предсказаний в кадрах (5)
- `--seq_len`: Длина буфера последовательности (64)
- `--camera`: ID камеры (0)

## Управление в реальном времени

- **'q'**: Выход из программы
- **'r'**: Сброс буферов (для перезапуска распознавания)

## Архитектура модели

### Входные данные
- Последовательность из 64 кадров
- Каждый кадр: 63 признака (21 точка × 3 координаты)
- Нормализация относительно запястья и размера кадра

### Модель
1. **Bidirectional LSTM** (2 слоя, 128 hidden units)
2. **Attention механизм** для фокусировки на важных кадрах
3. **Fully Connected слои** с dropout для классификации

### Выходные данные
- Вероятности для каждого класса жестов
- Сглаживание предсказаний по времени

## Оптимизация производительности

### GPU ускорение
```bash
# Проверка доступности CUDA
python -c "import torch; print(torch.cuda.is_available())"
```

### Настройка для слабых устройств
- Уменьшите `SEQ_LEN` до 32
- Уменьшите `HIDDEN_SIZE` до 64
- Увеличьте `prediction_interval` до 10
- Уменьшите разрешение камеры

## Качество и точность

### Факторы, влияющие на точность:
1. **Баланс классов**: Убедитесь, что у каждого жеста достаточно примеров
2. **Качество видео**: Хорошее освещение, четкие движения рук
3. **Консистентность**: Одинаковые условия съемки
4. **Длина жестов**: Динамические жесты должны быть полными

### Мониторинг качества:
```bash
# Проверка статистики классов
cat workdir/class_statistics.csv
cat workdir/sequences_statistics.csv
```

## Устранение проблем

### Ошибка "Модель не найдена"
```bash
# Убедитесь, что модель обучена
ls models/bilstm_best.pt
```

### Низкая точность
1. Проверьте баланс классов
2. Увеличьте количество эпох обучения
3. Настройте гиперпараметры
4. Проверьте качество данных

### Медленная работа
1. Используйте GPU если доступно
2. Уменьшите размер модели
3. Увеличьте интервал предсказаний
4. Уменьшите разрешение камеры

### Нестабильные предсказания
1. Увеличьте порог уверенности
2. Увеличьте размер буфера сглаживания
3. Улучшите освещение
4. Стабилизируйте камеру

## Расширение системы

### Добавление новых жестов
1. Добавьте видео в папку `train/`
2. Обновите `annotations.csv`
3. Переобучите модель

### Двуручные жесты
Измените `max_num_hands=2` в MediaPipe и адаптируйте нормализацию

### Экспорт модели
```python
# Экспорт в ONNX для развертывания
torch.onnx.export(model, dummy_input, "model.onnx")
```

## Лицензия

Этот проект создан для образовательных целей и исследований в области распознавания жестов.# sign_language_recognition

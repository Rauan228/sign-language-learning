# -*- coding: utf-8 -*-
import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import json
import matplotlib.pyplot as plt
from tqdm import tqdm
import cv2
import mediapipe as mp
import pandas as pd
from pathlib import Path
from gesture_lstm_model import GestureLSTM

class AllClassesGestureDataset(Dataset):
    def __init__(self, sequences, labels, scaler=None):
        self.sequences = sequences
        self.labels = labels
        self.scaler = scaler
        
    def __len__(self):
        return len(self.sequences)
    
    def __getitem__(self, idx):
        sequence = self.sequences[idx]
        label = self.labels[idx]
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –µ—Å–ª–∏ –µ—Å—Ç—å scaler
        if self.scaler is not None:
            original_shape = sequence.shape
            sequence_flat = sequence.reshape(-1, sequence.shape[-1])
            sequence_normalized = self.scaler.transform(sequence_flat)
            sequence = sequence_normalized.reshape(original_shape)
        
        return torch.FloatTensor(sequence), torch.LongTensor([label])

def extract_hand_keypoints(video_path):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫ —Ä—É–∫ –∏–∑ –≤–∏–¥–µ–æ"""
    mp_hands = mp.solutions.hands
    hands = mp_hands.Hands(
        static_image_mode=False,
        max_num_hands=2,
        min_detection_confidence=0.7,
        min_tracking_confidence=0.5
    )
    
    cap = cv2.VideoCapture(video_path)
    keypoints_sequence = []
    
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
            
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = hands.process(frame_rgb)
        
        frame_keypoints = []
        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                for landmark in hand_landmarks.landmark:
                    frame_keypoints.extend([landmark.x, landmark.y, landmark.z])
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫ –¥–æ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
        if len(frame_keypoints) == 0:
            # –ï—Å–ª–∏ —Ä—É–∫–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã, –∑–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏
            frame_keypoints = [0.0] * 126  # 21 —Ç–æ—á–∫–∞ * 3 –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã * 2 —Ä—É–∫–∏
        elif len(frame_keypoints) == 63:
            # –û–¥–Ω–∞ —Ä—É–∫–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞, –¥–æ–±–∞–≤–ª—è–µ–º –Ω—É–ª–∏ –¥–ª—è –≤—Ç–æ—Ä–æ–π —Ä—É–∫–∏
            frame_keypoints.extend([0.0] * 63)
        elif len(frame_keypoints) > 126:
            # –ï—Å–ª–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ –±–æ–ª—å—à–µ —Ç–æ—á–µ–∫, –æ–±—Ä–µ–∑–∞–µ–º –¥–æ 126
            frame_keypoints = frame_keypoints[:126]
        elif len(frame_keypoints) < 126 and len(frame_keypoints) > 63:
            # –ï—Å–ª–∏ –º–µ–∂–¥—É 63 –∏ 126, –¥–æ–ø–æ–ª–Ω—è–µ–º –¥–æ 126
            frame_keypoints.extend([0.0] * (126 - len(frame_keypoints)))
        
        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —É –Ω–∞—Å —Ä–æ–≤–Ω–æ 126 —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        frame_keypoints = frame_keypoints[:126] + [0.0] * max(0, 126 - len(frame_keypoints))
        
        keypoints_sequence.append(frame_keypoints)
    
    cap.release()
    hands.close()
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ numpy array —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ñ–æ—Ä–º—ã
    if len(keypoints_sequence) > 0:
        return np.array(keypoints_sequence, dtype=np.float32)
    else:
        return np.array([], dtype=np.float32).reshape(0, 126)

def load_no_event_data(test_dir, annotations_file, max_samples=50):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö no_event –∏–∑ –ø–∞–ø–∫–∏ test"""
    print("=== –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö no_event ===")
    
    sequences = []
    labels = []
    
    # –ß–∏—Ç–∞–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
    try:
        df = pd.read_csv(annotations_file, sep='\t', on_bad_lines='skip')
    except:
        try:
            df = pd.read_csv(annotations_file, sep=',', on_bad_lines='skip')
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π: {e}")
            return [], []
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º no_event –∑–∞–ø–∏—Å–∏
    no_event_records = df[df['text'] == 'no_event'].head(max_samples)
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(no_event_records)} –∑–∞–ø–∏—Å–µ–π no_event")
    
    processed_count = 0
    for _, row in no_event_records.iterrows():
        attachment_id = str(row['attachment_id'])
        
        # –£–±–∏—Ä–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å 'no' –µ—Å–ª–∏ –µ—Å—Ç—å
        if attachment_id.startswith('no'):
            file_id = attachment_id[2:]
        else:
            file_id = attachment_id
            
        video_path = os.path.join(test_dir, f"{file_id}.mp4")
        
        if os.path.exists(video_path):
            print(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é no_event: {video_path}")
            keypoints = extract_hand_keypoints(video_path)
            
            if len(keypoints) > 0:
                # –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã
                sequence_length = 30
                
                if len(keypoints) >= sequence_length:
                    # –ï—Å–ª–∏ –≤–∏–¥–µ–æ –¥–ª–∏–Ω–Ω–µ–µ, –±–µ—Ä–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏—Ö—Å—è –æ–∫–æ–Ω
                    step = max(1, len(keypoints) // 3)  # 3 –æ–∫–Ω–∞ –Ω–∞ –≤–∏–¥–µ–æ
                    for start_idx in range(0, len(keypoints) - sequence_length + 1, step):
                        sequence = keypoints[start_idx:start_idx + sequence_length]
                        sequences.append(sequence)
                        labels.append(6)  # no_event –±—É–¥–µ—Ç –∫–ª–∞—Å—Å 6
                else:
                    # –ï—Å–ª–∏ –≤–∏–¥–µ–æ –∫–æ—Ä–æ—á–µ, –¥–æ–ø–æ–ª–Ω—è–µ–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∫–∞–¥—Ä–∞
                    padded_sequence = np.zeros((sequence_length, keypoints.shape[1]))
                    padded_sequence[:len(keypoints)] = keypoints
                    if len(keypoints) > 0:
                        padded_sequence[len(keypoints):] = keypoints[-1]
                    sequences.append(padded_sequence)
                    labels.append(6)  # no_event –±—É–¥–µ—Ç –∫–ª–∞—Å—Å 6
                
                processed_count += 1
                print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ no_event {video_path}: {len(keypoints)} –∫–∞–¥—Ä–æ–≤")
        else:
            print(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {video_path}")
    
    print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed_count} —Ñ–∞–π–ª–æ–≤ no_event")
    return sequences, labels

def load_gesture_data(train_dir):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∂–µ—Å—Ç–æ–≤ –∏–∑ –ø–∞–ø–æ–∫"""
    sequences = []
    labels = []
    # –í—Å–µ –∫–ª–∞—Å—Å—ã: 6 –∂–µ—Å—Ç–æ–≤ + no_event
    class_names = ['–≤—ã—Ö–æ–¥–Ω—ã–µ', '–∫—É—Å–æ–∫', '–ª—é–±–∏—Ç—å', '–Ω–∞–∫–æ–Ω–µ—Ü-—Ç–æ', '–Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π', '–æ—Å–µ–Ω—å']
    class_to_idx = {name: idx for idx, name in enumerate(class_names)}
    # no_event –±—É–¥–µ—Ç –∏–º–µ—Ç—å –∏–Ω–¥–µ–∫—Å 6
    class_to_idx['no_event'] = 6
    class_names.append('no_event')
    idx_to_class = {idx: name for idx, name in enumerate(class_names)}
    
    print("=== –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∂–µ—Å—Ç–æ–≤ ===")
    
    for class_name in class_names[:-1]:  # –ò—Å–∫–ª—é—á–∞–µ–º no_event, –µ–≥–æ –∑–∞–≥—Ä—É–∂–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ
        class_dir = os.path.join(train_dir, class_name)
        if not os.path.exists(class_dir):
            print(f"–ü–∞–ø–∫–∞ {class_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
            continue
            
        print(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –∫–ª–∞—Å—Å '{class_name}'...")
        
        for video_file in os.listdir(class_dir):
            if video_file.endswith('.mp4'):
                video_path = os.path.join(class_dir, video_file)
                print(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é {video_path}...")
                
                keypoints = extract_hand_keypoints(video_path)
                if len(keypoints) > 0:
                    # –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã
                    sequence_length = 30
                    
                    if len(keypoints) >= sequence_length:
                        # –ï—Å–ª–∏ –≤–∏–¥–µ–æ –¥–ª–∏–Ω–Ω–µ–µ, –±–µ—Ä–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏—Ö—Å—è –æ–∫–æ–Ω
                        step = max(1, len(keypoints) // 5)  # 5 –æ–∫–æ–Ω –Ω–∞ –≤–∏–¥–µ–æ
                        for start_idx in range(0, len(keypoints) - sequence_length + 1, step):
                            sequence = keypoints[start_idx:start_idx + sequence_length]
                            sequences.append(sequence)
                            labels.append(class_to_idx[class_name])
                    else:
                        # –ï—Å–ª–∏ –≤–∏–¥–µ–æ –∫–æ—Ä–æ—á–µ, –¥–æ–ø–æ–ª–Ω—è–µ–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∫–∞–¥—Ä–∞
                        padded_sequence = np.zeros((sequence_length, keypoints.shape[1]))
                        padded_sequence[:len(keypoints)] = keypoints
                        if len(keypoints) > 0:
                            padded_sequence[len(keypoints):] = keypoints[-1]
                        sequences.append(padded_sequence)
                        labels.append(class_to_idx[class_name])
                        
                    print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {video_path}: {len(keypoints)} –∫–∞–¥—Ä–æ–≤")
    
    return sequences, labels, class_to_idx, idx_to_class

def augment_data(sequences, labels, target_samples_per_class=20):
    """–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤"""
    print("\n=== –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö ===")
    
    augmented_sequences = []
    augmented_labels = []
    
    unique_labels = np.unique(labels)
    
    for label in unique_labels:
        class_sequences = [seq for seq, lbl in zip(sequences, labels) if lbl == label]
        class_labels = [lbl for lbl in labels if lbl == label]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        augmented_sequences.extend(class_sequences)
        augmented_labels.extend(class_labels)
        
        # –ï—Å–ª–∏ –Ω—É–∂–Ω–æ –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç—Ç–æ–≥–æ –∫–ª–∞—Å—Å–∞
        current_count = len(class_sequences)
        if current_count < target_samples_per_class:
            needed = target_samples_per_class - current_count
            
            for _ in range(needed):
                # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–∑ –∫–ª–∞—Å—Å–∞
                idx = np.random.randint(0, len(class_sequences))
                original_seq = class_sequences[idx].copy()
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é
                augmented_seq = original_seq.copy()
                
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à–æ–π —à—É–º
                noise = np.random.normal(0, 0.01, augmented_seq.shape)
                augmented_seq += noise
                
                # –ù–µ–±–æ–ª—å—à–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
                scale_factor = np.random.uniform(0.95, 1.05)
                augmented_seq *= scale_factor
                
                augmented_sequences.append(augmented_seq)
                augmented_labels.append(label)
    
    augmented_sequences = np.array(augmented_sequences)
    augmented_labels = np.array(augmented_labels)
    
    print("–ü–æ—Å–ª–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏:")
    print(f"–í—Å–µ–≥–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π: {len(augmented_sequences)}")
    for label in unique_labels:
        count = sum(1 for l in augmented_labels if l == label)
        print(f"–ö–ª–∞—Å—Å {label}: {count}")
    
    return augmented_sequences, augmented_labels

def train_model(model, train_loader, val_loader, num_epochs=50, device='cuda'):
    """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
    print("\n=== –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è ===")
    
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=5, factor=0.5)
    
    train_losses = []
    train_accuracies = []
    val_losses = []
    val_accuracies = []
    
    best_val_acc = 0.0
    early_stop_threshold = 0.95  # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ 95% —Ç–æ—á–Ω–æ—Å—Ç–∏
    
    for epoch in range(num_epochs):
        # –û–±—É—á–µ–Ω–∏–µ
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        train_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')
        for batch_idx, (data, target) in enumerate(train_bar):
            data, target = data.to(device), target.squeeze().to(device)
            
            optimizer.zero_grad()
            output, _ = model(data)  # –ú–æ–¥–µ–ª—å –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç (output, attention_weights)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            train_total += target.size(0)
            train_correct += (predicted == target).sum().item()
            
            train_bar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{100.*train_correct/train_total:.2f}%'
            })
        
        train_acc = 100. * train_correct / train_total
        train_loss = train_loss / len(train_loader)
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            val_bar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]')
            for data, target in val_bar:
                data, target = data.to(device), target.squeeze().to(device)
                output, _ = model(data)  # –ú–æ–¥–µ–ª—å –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç (output, attention_weights)
                loss = criterion(output, target)
                
                val_loss += loss.item()
                _, predicted = torch.max(output.data, 1)
                val_total += target.size(0)
                val_correct += (predicted == target).sum().item()
                
                val_bar.set_postfix({
                    'Loss': f'{loss.item():.4f}',
                    'Acc': f'{100.*val_correct/val_total:.2f}%'
                })
        
        val_acc = 100. * val_correct / val_total
        val_loss = val_loss / len(val_loader)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        train_losses.append(train_loss)
        train_accuracies.append(train_acc)
        val_losses.append(val_loss)
        val_accuracies.append(val_acc)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º learning rate
        scheduler.step(val_acc)
        
        print(f'Epoch [{epoch+1}/{num_epochs}]')
        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')
        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')
        print('-' * 50)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_acc': val_acc,
                'val_loss': val_loss
            }, 'best_all_classes_model.pth')
            print(f'–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –Ω–æ–≤–∞—è –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å —Ç–æ—á–Ω–æ—Å—Ç—å—é: {val_acc:.2f}%')
        
        # –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ 95% —Ç–æ—á–Ω–æ—Å—Ç–∏
        if val_acc >= early_stop_threshold * 100:
            print(f'\n–î–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ —Ç–æ—á–Ω–æ—Å—Ç—å {val_acc:.2f}% (>= {early_stop_threshold*100:.0f}%). –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ.')
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_acc': val_acc,
                'val_loss': val_loss
            }, 'best_all_classes_model.pth')
            print(f'–§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ —Å —Ç–æ—á–Ω–æ—Å—Ç—å—é: {val_acc:.2f}%')
            break
    
    return train_losses, train_accuracies, val_losses, val_accuracies, best_val_acc

def plot_training_history(train_losses, train_accuracies, val_losses, val_accuracies, save_path):
    """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –æ–±—É—á–µ–Ω–∏—è"""
    fig, ((ax1, ax2)) = plt.subplots(1, 2, figsize=(15, 5))
    
    # –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å
    ax1.plot(train_losses, label='Train Loss', color='blue')
    ax1.plot(val_losses, label='Validation Loss', color='red')
    ax1.set_title('–ü–æ—Ç–µ—Ä–∏ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è')
    ax1.set_xlabel('–≠–ø–æ—Ö–∞')
    ax1.set_ylabel('–ü–æ—Ç–µ—Ä–∏')
    ax1.legend()
    ax1.grid(True)
    
    # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏
    ax2.plot(train_accuracies, label='Train Accuracy', color='blue')
    ax2.plot(val_accuracies, label='Validation Accuracy', color='red')
    ax2.set_title('–¢–æ—á–Ω–æ—Å—Ç—å –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è')
    ax2.set_xlabel('–≠–ø–æ—Ö–∞')
    ax2.set_ylabel('–¢–æ—á–Ω–æ—Å—Ç—å (%)')
    ax2.legend()
    ax2.grid(True)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"–ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {save_path}")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("–°–∫—Ä–∏–ø—Ç –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è –≤—Å–µ—Ö –∫–ª–∞—Å—Å–æ–≤ –∂–µ—Å—Ç–æ–≤ + no_event")
    print("–ö–ª–∞—Å—Å—ã: –≤—ã—Ö–æ–¥–Ω—ã–µ, –∫—É—Å–æ–∫, –ª—é–±–∏—Ç—å, –Ω–∞–∫–æ–Ω–µ—Ü-—Ç–æ, –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π, –æ—Å–µ–Ω—å, no_event")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
    train_dir = 'D:/gesture/train'
    test_dir = 'D:/gesture/test'
    annotations_file = 'D:/gesture/annotations.csv'
    output_dir = 'D:/gesture/all_classes_model_output'
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –≤—ã—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    os.makedirs(output_dir, exist_ok=True)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∂–µ—Å—Ç–æ–≤
    gesture_sequences, gesture_labels, class_to_idx, idx_to_class = load_gesture_data(train_dir)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö no_event
    no_event_sequences, no_event_labels = load_no_event_data(test_dir, annotations_file, max_samples=50)
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
    all_sequences = gesture_sequences + no_event_sequences
    all_labels = gesture_labels + no_event_labels
    
    if len(all_sequences) == 0:
        print("–û—à–∏–±–∫–∞: –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ!")
        return
    
    print(f"\n–í—Å–µ–≥–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π: {len(all_sequences)}")
    for class_name, class_idx in class_to_idx.items():
        count = sum(1 for label in all_labels if label == class_idx)
        print(f"–ö–ª–∞—Å—Å '{class_name}': {count}")
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ numpy arrays
    sequences = np.array(all_sequences)
    labels = np.array(all_labels)
    
    print(f"\n–§–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö: {sequences.shape}")
    print(f"–§–æ—Ä–º–∞ –º–µ—Ç–æ–∫: {labels.shape}")
    
    # –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    sequences, labels = augment_data(sequences, labels, target_samples_per_class=30)
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    print("\n=== –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö ===")
    original_shape = sequences.shape
    sequences_flat = sequences.reshape(-1, sequences.shape[-1])
    
    scaler = StandardScaler()
    sequences_normalized = scaler.fit_transform(sequences_flat)
    sequences = sequences_normalized.reshape(original_shape)
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫–∏
    X_train, X_val, y_train, y_val = train_test_split(
        sequences, labels, test_size=0.2, random_state=42, stratify=labels
    )
    
    print(f"\n–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train.shape}")
    print(f"–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_val.shape}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    train_dataset = AllClassesGestureDataset(X_train, y_train)
    val_dataset = AllClassesGestureDataset(X_val, y_val)
    
    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print("\n=== –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ ===")
    input_size = sequences.shape[-1]  # 126 (21 —Ç–æ—á–∫–∞ * 3 –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã * 2 —Ä—É–∫–∏)
    hidden_size = 128
    num_layers = 2
    num_classes = len(class_to_idx)  # 7 –∫–ª–∞—Å—Å–æ–≤
    
    model = GestureLSTM(input_size, hidden_size, num_layers, num_classes).to(device)
    print(f"–ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞ —Å {num_classes} –∫–ª–∞—Å—Å–∞–º–∏")
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    train_losses, train_accuracies, val_losses, val_accuracies, best_val_acc = train_model(
        model, train_loader, val_loader, num_epochs=50, device=device
    )
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
    checkpoint = torch.load('best_all_classes_model.pth')
    model.load_state_dict(checkpoint['model_state_dict'])
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    print("\n=== –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ ===")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
    torch.save({
        'model_state_dict': model.state_dict(),
        'scaler_mean': scaler.mean_,
        'scaler_scale': scaler.scale_,
        'class_to_idx': class_to_idx,
        'idx_to_class': idx_to_class,
        'input_size': input_size,
        'hidden_size': hidden_size,
        'num_layers': num_layers,
        'num_classes': num_classes,
        'best_accuracy': best_val_acc
    }, os.path.join(output_dir, 'all_classes_gesture_model.pth'))
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    config = {
        'class_to_idx': class_to_idx,
        'idx_to_class': idx_to_class,
        'input_size': input_size,
        'hidden_size': hidden_size,
        'num_layers': num_layers,
        'num_classes': num_classes,
        'best_accuracy': best_val_acc,
        'scaler_mean': scaler.mean_.tolist(),
        'scaler_scale': scaler.scale_.tolist()
    }
    
    with open(os.path.join(output_dir, 'config.json'), 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤
    plot_training_history(
        train_losses, train_accuracies, val_losses, val_accuracies,
        os.path.join(output_dir, 'training_history.png')
    )
    
    print(f"\nüéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"–õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_val_acc:.2f}%")
    print(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {output_dir}")
    print(f"–ö–ª–∞—Å—Å—ã: {list(class_to_idx.keys())}")

if __name__ == '__main__':
    main()